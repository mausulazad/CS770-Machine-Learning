{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Data Generation:\n",
        "   - Generate two normal distributions each of size 30,000 with feature dimension=100.\n",
        "   - You may choose arbitrary mean and standard deviation.\n",
        "   - Label the first set of feature vectors as \"+1\" and the second set of feature vectors as either \"-1\" or \"0.\"\n"
      ],
      "metadata": {
        "id": "auHo82NVj2Pt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "75475e1a5a876ae536b487f30131fda9",
          "grade": true,
          "grade_id": "cell-3e53b094549fc43b",
          "locked": false,
          "points": 10,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "LX_2p8B-zUfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a22e1bf-82e4-45cf-adf0-fd132b796b7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 101)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "X_1 = np.random.normal(loc=0.0, size=(30000, 100))\n",
        "X_2 = np.random.normal(loc=3.0, size=(30000, 100))\n",
        "X = np.concatenate((X_1, X_2), axis=0)\n",
        "#X = np.random.normal(size=(2*30000,100))\n",
        "\n",
        "positive_y = np.ones((30000,1))\n",
        "negative_y = np.zeros((30000,1))\n",
        "y = np.concatenate((positive_y, negative_y), axis=0)\n",
        "\n",
        "data = np.concatenate((X, y), axis=1)\n",
        "\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7c0f513197bb21a671dd6360285dd470",
          "grade": false,
          "grade_id": "cell-2396855d1ce43de8",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "zSWVZgoBzUfe"
      },
      "source": [
        "### 2. Data Splitting\n",
        "- Split the generated data into training, validation and test sets in 8:1:1 ratio.\n",
        "- You are expected to write the code from scratch. (i.e. Do not use train_test_split function from sklearn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6b88f1edd128e8e668e91b77d522bc73",
          "grade": true,
          "grade_id": "cell-07bca55242c6add3",
          "locked": false,
          "points": 10,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "e765LajtzUfm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f5f373-bd45-4ca1-f86f-ae2972b50473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48000 True\n",
            "6000 True\n",
            "6000 True\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "def split_train_val_test(data, val_ratio=0.1, test_ratio=0.1):\n",
        "    shuffled_indices = np.random.permutation(len(data))\n",
        "    val_set_size = int((len(data) * val_ratio))\n",
        "    test_set_size = int((len(data) * test_ratio))\n",
        "    val_indices = shuffled_indices[:val_set_size]\n",
        "    test_indices = shuffled_indices[val_set_size:(val_set_size + test_set_size)]\n",
        "    train_indices = shuffled_indices[(val_set_size + test_set_size):]\n",
        "    return data[train_indices], data[val_indices], data[test_indices]\n",
        "\n",
        "train_set, val_set, test_set = split_train_val_test(data)\n",
        "\n",
        "print(len(train_set), len(train_set) == 0.8 * len(data))\n",
        "print(len(val_set), len(val_set) == 0.1 * len(data))\n",
        "print(len(test_set), len(test_set) == 0.1 * len(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "874cd58cc22c38052eb0325903dd1dd8",
          "grade": false,
          "grade_id": "cell-87cec8097abc0d13",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "3o8iJibmzUfp"
      },
      "source": [
        "### 3. Classifier Implementation\n",
        "- Implement Linear Classifiers using three different methods.\n",
        "  - Random Method\n",
        "  - Perceptron Method\n",
        "  - Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9124a4169ec297722feaf5492e094b56",
          "grade": true,
          "grade_id": "cell-5dc7f3ee5f957b96",
          "locked": false,
          "points": 20,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "EWK3xoZkzUfr"
      },
      "outputs": [],
      "source": [
        "class RandomLinearClassifier:\n",
        "    def __init__(self, train_set, val_set=None, test_set=None, k=100, dim=100):\n",
        "        self.train_set = train_set\n",
        "        self.val_set = val_set\n",
        "        self.test_set = test_set\n",
        "        self.k = k\n",
        "        self.dim = dim\n",
        "        self.best_weights_and_bias = None\n",
        "\n",
        "    def train(self):\n",
        "        best_loss_score = np.inf\n",
        "        features = self.train_set[:, :self.dim]\n",
        "        labels = self.train_set[:, self.dim]\n",
        "        for i in range(self.k):\n",
        "            # No heuristic is followed for choosing weights, random selection is taking place\n",
        "            weights_and_bias = np.random.rand(self.dim + 1)\n",
        "            weighted_sum = np.dot(features, weights_and_bias[:self.dim]) + weights_and_bias[self.dim]\n",
        "            predictions = (weighted_sum > 0).astype(float)\n",
        "            loss_score = np.mean(predictions == labels)\n",
        "            print(f\"Epoch: {i}, Loss: {loss_score}\")\n",
        "            if(loss_score < best_loss_score):\n",
        "                best_loss_score = loss_score\n",
        "                self.best_weights_and_bias = weights_and_bias\n",
        "\n",
        "    def test(self):\n",
        "        features = self.test_set[:, :self.dim]\n",
        "        labels = self.test_set[:, self.dim]\n",
        "        weighted_sum = np.dot(features, self.best_weights_and_bias[:self.dim]) + self.best_weights_and_bias[self.dim]\n",
        "        predictions = (weighted_sum > 0).astype(float)\n",
        "        return predictions, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a2b4b4fba3fbe9540427b5a4bce6f5ba",
          "grade": true,
          "grade_id": "cell-9f14a9d375d7dac3",
          "locked": false,
          "points": 20,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "DRBxB_H8zUfs"
      },
      "outputs": [],
      "source": [
        "class PerceptronClassifier:\n",
        "    def __init__(self, train_set, val_set=None, test_set=None, tau=100, dim=100):\n",
        "        self.train_set = train_set\n",
        "        self.val_set = val_set\n",
        "        self.test_set = test_set\n",
        "        self.tau = tau\n",
        "        self.dim = dim\n",
        "        self.weights_and_bias = np.zeros(self.dim + 1)\n",
        "\n",
        "    def train(self):\n",
        "        features = self.train_set[:, :self.dim]\n",
        "        labels = self.train_set[:, self.dim]\n",
        "\n",
        "        for _ in range(self.tau):\n",
        "            # Weights are getting checked for each training instance\n",
        "            for i, (x, y) in enumerate(zip(features, labels)):\n",
        "                weighted_sum = np.dot(x, self.weights_and_bias[:self.dim]) + self.weights_and_bias[self.dim]\n",
        "                if (y * weighted_sum) <= 0.0:\n",
        "                    self.weights_and_bias[:self.dim] += (y * x)\n",
        "                    self.weights_and_bias[self.dim] += y\n",
        "\n",
        "    def test(self):\n",
        "        features = self.test_set[:, :self.dim]\n",
        "        labels = self.test_set[:, self.dim]\n",
        "        weighted_sum = np.dot(features, self.weights_and_bias[:self.dim]) + self.weights_and_bias[self.dim]\n",
        "        predictions = (weighted_sum > 0).astype(float)\n",
        "        return predictions, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c460fdd678caf0a0eec35fccef2e90e5",
          "grade": true,
          "grade_id": "cell-d785f958fbeaf1d2",
          "locked": false,
          "points": 20,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "L7UHELKAzUft"
      },
      "outputs": [],
      "source": [
        "class GradientBasedClassifier:\n",
        "    def __init__(self, train_set, val_set=None, test_set=None, lr=0.01, epsilon=0.0002, regularizer=0.1, dim=100, k_fold=False):\n",
        "        self.train_set = train_set\n",
        "        self.val_set = val_set\n",
        "        self.test_set = test_set\n",
        "        self.lr = lr\n",
        "        self.epsilon = epsilon\n",
        "        self.regularizer = regularizer\n",
        "        self.dim = dim\n",
        "        self.weights_and_bias = np.random.rand(self.dim + 1)\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        # To avoid overflow error a different form of sigmoid function is used\n",
        "        # Link (last answer): https://stackoverflow.com/questions/40726490/overflow-error-in-pythons-numpy-exp-function\n",
        "        #return np.where(z >= 0.0, 1 / (1 + np.exp(-z)), np.exp(z) / (1 + np.exp(z)))\n",
        "        return np.clip(np.exp(z) / (1 + np.exp(z)), 1e-15, 1 - 1e-15)\n",
        "\n",
        "    def train(self):\n",
        "        features = self.train_set[:, :self.dim]\n",
        "        labels = self.train_set[:, self.dim]\n",
        "\n",
        "        i = 0\n",
        "        while(True):\n",
        "            weighted_sum = np.dot(features, self.weights_and_bias[:self.dim]) + self.weights_and_bias[self.dim]\n",
        "            #predictions = np.where(predictions == 0.0, 1e-15, predictions)\n",
        "            #predictions = np.where(predictions == 1.0, 0.999999999999, predictions)\n",
        "            predictions = self.sigmoid(weighted_sum)\n",
        "            weight_gradient = (np.dot(features.T, (predictions - labels)) / len(labels)) + (self.regularizer * self.weights_and_bias[:self.dim])\n",
        "            bias_gradient = np.mean(predictions - labels)\n",
        "\n",
        "            current_val_loss = self.eval()\n",
        "\n",
        "            self.weights_and_bias[:self.dim] -= (self.lr * weight_gradient)\n",
        "            self.weights_and_bias[self.dim] -= (self.lr * bias_gradient)\n",
        "\n",
        "            updated_val_loss = self.eval()\n",
        "\n",
        "            # For keeping better track of training progress\n",
        "            if(i % 100 == 0):\n",
        "                print(f\"Epoch: {i}, Loss: {updated_val_loss}\")\n",
        "\n",
        "            # Training should be stopped once the loss difference is close to zero and less than epsilon\n",
        "            if(abs(updated_val_loss - current_val_loss) < self.epsilon):\n",
        "                break\n",
        "            i += 1\n",
        "\n",
        "    def test(self):\n",
        "        features = self.test_set[:, :self.dim]\n",
        "        labels = self.test_set[:, self.dim]\n",
        "        weighted_sum = np.dot(features, self.weights_and_bias[:self.dim]) + self.weights_and_bias[self.dim]\n",
        "        predictions = self.sigmoid(weighted_sum)\n",
        "        # Based on problem type, different thresholds can be used\n",
        "        predictions = (predictions >= 0.5).astype(float)\n",
        "        return predictions, labels\n",
        "\n",
        "    def eval(self):\n",
        "        features = self.val_set[:, :self.dim]\n",
        "        labels = self.val_set[:, self.dim]\n",
        "        weighted_sum = np.dot(features, self.weights_and_bias[:self.dim]) + self.weights_and_bias[self.dim]\n",
        "        predictions = self.sigmoid(weighted_sum)\n",
        "        loss = -np.mean((labels * np.log(predictions)) + ((1 - labels) * np.log(1 - predictions))) + ((self.regularizer / 2.0) * (np.linalg.norm(self.weights_and_bias[:self.dim]) ** 2))\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "10c6379f3912f3e8f4c726b058bd990a",
          "grade": false,
          "grade_id": "cell-78578cf2183745d4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ddyihhj8zUfv"
      },
      "source": [
        "### 4. Performance Evaluation\n",
        "- Evaluate the performance of each method on the test set using various evaluation metrics such as accuracy, precision, recall, F1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8988d9cdb01045ee5857eb2aa18aec07",
          "grade": true,
          "grade_id": "cell-17cf1c1fc903c5fd",
          "locked": false,
          "points": 10,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "J5CWlXGtzUfy"
      },
      "outputs": [],
      "source": [
        "def measure_performance(preds, labels):\n",
        "    correct = 0\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "\n",
        "    for (pred, label) in zip(preds, labels):\n",
        "        if (pred == label):\n",
        "            if (pred == 1.0 and label ==1.0):\n",
        "                tp += 1\n",
        "            correct += 1\n",
        "        else:\n",
        "            if (pred == 1.0 and label == 0.0):\n",
        "                fp += 1\n",
        "            if (pred == 0.0 and label == 1.0):\n",
        "                fn += 1\n",
        "\n",
        "    accuracy = correct / len(labels)\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    return accuracy, precision, recall, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Linear Classifier\n",
        "rlc = RandomLinearClassifier(train_set, val_set, test_set)\n",
        "rlc.train()\n",
        "predictions, labels = rlc.test()\n",
        "accuracy, precision, recall, f1_score = measure_performance(predictions, labels)\n",
        "print(f\"Accuracy for Random Linear Classifier: {accuracy}\")\n",
        "print(f\"Precision for Random Linear Classifier: {precision}\")\n",
        "print(f\"Recall for Random Linear Classifier: {recall}\")\n",
        "print(f\"F-1 score for Random Linear Classifier: {f1_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIuKs4VXkHET",
        "outputId": "ae0889e4-e458-4ef9-a3b1-75e5aac5d35d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 0.2684166666666667\n",
            "Epoch: 1, Loss: 0.2528125\n",
            "Epoch: 2, Loss: 0.2775625\n",
            "Epoch: 3, Loss: 0.2596041666666667\n",
            "Epoch: 4, Loss: 0.25514583333333335\n",
            "Epoch: 5, Loss: 0.26195833333333335\n",
            "Epoch: 6, Loss: 0.2581875\n",
            "Epoch: 7, Loss: 0.27295833333333336\n",
            "Epoch: 8, Loss: 0.2605625\n",
            "Epoch: 9, Loss: 0.2795\n",
            "Epoch: 10, Loss: 0.2696875\n",
            "Epoch: 11, Loss: 0.282375\n",
            "Epoch: 12, Loss: 0.2800625\n",
            "Epoch: 13, Loss: 0.2748125\n",
            "Epoch: 14, Loss: 0.2683125\n",
            "Epoch: 15, Loss: 0.28227083333333336\n",
            "Epoch: 16, Loss: 0.24983333333333332\n",
            "Epoch: 17, Loss: 0.2766875\n",
            "Epoch: 18, Loss: 0.2720208333333333\n",
            "Epoch: 19, Loss: 0.2532083333333333\n",
            "Epoch: 20, Loss: 0.2713541666666667\n",
            "Epoch: 21, Loss: 0.2685625\n",
            "Epoch: 22, Loss: 0.2622291666666667\n",
            "Epoch: 23, Loss: 0.2623125\n",
            "Epoch: 24, Loss: 0.2634166666666667\n",
            "Epoch: 25, Loss: 0.277125\n",
            "Epoch: 26, Loss: 0.28539583333333335\n",
            "Epoch: 27, Loss: 0.2622291666666667\n",
            "Epoch: 28, Loss: 0.25425\n",
            "Epoch: 29, Loss: 0.24933333333333332\n",
            "Epoch: 30, Loss: 0.25729166666666664\n",
            "Epoch: 31, Loss: 0.2743333333333333\n",
            "Epoch: 32, Loss: 0.255625\n",
            "Epoch: 33, Loss: 0.28520833333333334\n",
            "Epoch: 34, Loss: 0.274375\n",
            "Epoch: 35, Loss: 0.2645416666666667\n",
            "Epoch: 36, Loss: 0.2785416666666667\n",
            "Epoch: 37, Loss: 0.2532916666666667\n",
            "Epoch: 38, Loss: 0.2630416666666667\n",
            "Epoch: 39, Loss: 0.25833333333333336\n",
            "Epoch: 40, Loss: 0.2773333333333333\n",
            "Epoch: 41, Loss: 0.25475\n",
            "Epoch: 42, Loss: 0.27060416666666665\n",
            "Epoch: 43, Loss: 0.270375\n",
            "Epoch: 44, Loss: 0.26460416666666664\n",
            "Epoch: 45, Loss: 0.27839583333333334\n",
            "Epoch: 46, Loss: 0.26945833333333336\n",
            "Epoch: 47, Loss: 0.2865625\n",
            "Epoch: 48, Loss: 0.259\n",
            "Epoch: 49, Loss: 0.26925\n",
            "Epoch: 50, Loss: 0.2785416666666667\n",
            "Epoch: 51, Loss: 0.277375\n",
            "Epoch: 52, Loss: 0.26239583333333333\n",
            "Epoch: 53, Loss: 0.25285416666666666\n",
            "Epoch: 54, Loss: 0.2509166666666667\n",
            "Epoch: 55, Loss: 0.2576875\n",
            "Epoch: 56, Loss: 0.28058333333333335\n",
            "Epoch: 57, Loss: 0.2677083333333333\n",
            "Epoch: 58, Loss: 0.274875\n",
            "Epoch: 59, Loss: 0.25483333333333336\n",
            "Epoch: 60, Loss: 0.260125\n",
            "Epoch: 61, Loss: 0.2835416666666667\n",
            "Epoch: 62, Loss: 0.282875\n",
            "Epoch: 63, Loss: 0.271125\n",
            "Epoch: 64, Loss: 0.28022916666666664\n",
            "Epoch: 65, Loss: 0.2545208333333333\n",
            "Epoch: 66, Loss: 0.266625\n",
            "Epoch: 67, Loss: 0.2604375\n",
            "Epoch: 68, Loss: 0.2669375\n",
            "Epoch: 69, Loss: 0.2729375\n",
            "Epoch: 70, Loss: 0.2513125\n",
            "Epoch: 71, Loss: 0.2592291666666667\n",
            "Epoch: 72, Loss: 0.2685208333333333\n",
            "Epoch: 73, Loss: 0.2705625\n",
            "Epoch: 74, Loss: 0.26429166666666665\n",
            "Epoch: 75, Loss: 0.27725\n",
            "Epoch: 76, Loss: 0.25729166666666664\n",
            "Epoch: 77, Loss: 0.278375\n",
            "Epoch: 78, Loss: 0.26529166666666665\n",
            "Epoch: 79, Loss: 0.2566875\n",
            "Epoch: 80, Loss: 0.25395833333333334\n",
            "Epoch: 81, Loss: 0.26960416666666664\n",
            "Epoch: 82, Loss: 0.24979166666666666\n",
            "Epoch: 83, Loss: 0.27989583333333334\n",
            "Epoch: 84, Loss: 0.2688333333333333\n",
            "Epoch: 85, Loss: 0.2804375\n",
            "Epoch: 86, Loss: 0.2548958333333333\n",
            "Epoch: 87, Loss: 0.2823333333333333\n",
            "Epoch: 88, Loss: 0.27089583333333334\n",
            "Epoch: 89, Loss: 0.27327083333333335\n",
            "Epoch: 90, Loss: 0.258125\n",
            "Epoch: 91, Loss: 0.253125\n",
            "Epoch: 92, Loss: 0.2607083333333333\n",
            "Epoch: 93, Loss: 0.2529166666666667\n",
            "Epoch: 94, Loss: 0.27610416666666665\n",
            "Epoch: 95, Loss: 0.27195833333333336\n",
            "Epoch: 96, Loss: 0.25879166666666664\n",
            "Epoch: 97, Loss: 0.2793541666666667\n",
            "Epoch: 98, Loss: 0.27445833333333336\n",
            "Epoch: 99, Loss: 0.25214583333333335\n",
            "Accuracy for Random Linear Classifier: 0.25616666666666665\n",
            "Precision for Random Linear Classifier: 0.33914386584289496\n",
            "Recall for Random Linear Classifier: 0.5114808652246257\n",
            "F-1 score for Random Linear Classifier: 0.40785458405201014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perceptron Classifier\n",
        "pc = PerceptronClassifier(train_set, val_set, test_set)\n",
        "pc.train()\n",
        "predictions, labels = pc.test()\n",
        "accuracy, precision, recall, f1_score = measure_performance(predictions, labels)\n",
        "print(f\"Accuracy for Perceptron Classifier: {accuracy}\")\n",
        "print(f\"Precision for Perceptron Classifier: {precision}\")\n",
        "print(f\"Recall for Perceptron Classifier: {recall}\")\n",
        "print(f\"F-1 score for Perceptron Classifier: {f1_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT3M9gwfkj58",
        "outputId": "88409f3e-29b1-4caf-e32d-c5447d049452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Perceptron Classifier: 0.5008333333333334\n",
            "Precision for Perceptron Classifier: 0.5008333333333334\n",
            "Recall for Perceptron Classifier: 1.0\n",
            "F-1 score for Perceptron Classifier: 0.6674069961132704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent Based Classifier\n",
        "gbc = GradientBasedClassifier(train_set, val_set, test_set)\n",
        "gbc.train()\n",
        "predictions, labels = gbc.test()\n",
        "accuracy, precision, recall, f1_score = measure_performance(predictions, labels)\n",
        "print(f\"Accuracy for Gradient Descent Based Classifier: {accuracy}\")\n",
        "print(f\"Precision for Gradient Descent Based Classifier: {precision}\")\n",
        "print(f\"Recall for Gradient Descent Based Classifier: {recall}\")\n",
        "print(f\"F-1 score for Gradient Descent based Classifier: {f1_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93X_-HSukmQb",
        "outputId": "3409f9ae-92c0-452c-c45f-5048e280d820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 20.275146815899042\n",
            "Epoch: 100, Loss: 0.7968716746469977\n",
            "Epoch: 200, Loss: 0.6115766447155084\n",
            "Epoch: 300, Loss: 0.46826736688042414\n",
            "Epoch: 400, Loss: 0.36012644300367863\n",
            "Epoch: 500, Loss: 0.28061129490096737\n",
            "Epoch: 600, Loss: 0.22323222772533236\n",
            "Epoch: 700, Loss: 0.1821379678680006\n",
            "Epoch: 800, Loss: 0.15257588644540213\n",
            "Accuracy for Gradient Descent Based Classifier: 0.9946666666666667\n",
            "Precision for Gradient Descent Based Classifier: 1.0\n",
            "Recall for Gradient Descent Based Classifier: 0.989351081530782\n",
            "F-1 score for Gradient Descent based Classifier: 0.9946470391435263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "b9ebf6cfa87902c9dcec1e5525d9f30e",
          "grade": false,
          "grade_id": "cell-29b516059de95027",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "8zG1g05SzUf2"
      },
      "source": [
        "## NOT COMPLETED: Cross Validation\n",
        "- Implement cross validation on gradient descent based classifier model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Validation\n",
        "Implement cross validation on gradient descent based classifier model"
      ],
      "metadata": {
        "id": "qaLy2o1liNz6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "de01db0e422cfc1c44737327e3ce20c7",
          "grade": true,
          "grade_id": "cell-7622b24794230687",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "P3MjmnmjzUf3"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def cross_validation(data, k=10):\n",
        "    fold_size = math.floor(data.shape[0] / k)\n",
        "\n",
        "    performance_metrics = {\n",
        "        'accuracy': list(),\n",
        "        'precision': list(),\n",
        "        'recall': list(),\n",
        "        'f1_score': list()\n",
        "    }\n",
        "\n",
        "    for i in range(k):\n",
        "        fold_starting_index = i * fold_size\n",
        "        fold_ending_index = (i+1) * fold_size\n",
        "        #print(fold_starting_index)\n",
        "        #print(fold_ending_index)\n",
        "        train_set = np.concatenate((data[:fold_starting_index], data[fold_ending_index:]), axis=0)\n",
        "        val_set = data[fold_starting_index:fold_ending_index]\n",
        "        #print(train_set.shape)\n",
        "        #print(val_set.shape)\n",
        "\n",
        "        gd_classifier = GradientBasedClassifier(train_set, test_set=val_set)\n",
        "        gd_classifier.train()\n",
        "        predictions, labels = gd_classifier.test()\n",
        "        accuracy, precision, recall, f1_score = measure_performance(predictions, labels)\n",
        "        performance_metrics['accuracy'].append(accuracy)\n",
        "        performance_metrics['precision'].append(precision)\n",
        "        performance_metrics['recall'].append(recall)\n",
        "        performance_metrics['f1_score'].append(f1_score)\n",
        "\n",
        "    performance_metrics['accuracy'] = sum(performance_metrics['accuracy']) / len(performance_metrics['accuracy'])\n",
        "    performance_metrics['precision'] = sum(performance_metrics['precision']) / len(performance_metrics['precision'])\n",
        "    performance_metrics['recall'] = sum(performance_metrics['recall']) / len(performance_metrics['recall'])\n",
        "    performance_metrics['f1_score'] = sum(performance_metrics['f1_score']) / len(performance_metrics['f1_score'])\n",
        "\n",
        "    return performance_metrics\n",
        "\n",
        "\n",
        "shuffled_indices = np.random.permutation(len(data))\n",
        "shuffled_row = shuffled_indices[:len(data)]\n",
        "shuffled_data = data[shuffled_row]\n",
        "\n",
        "print(cross_validation(data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XhDLgqePmYUT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}